image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0@sha256:9941689d599c154f15bf88d9ae7942d388909a2711ead51b8534f7ee3487f869
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.24.2-ffmpeg-core@sha256:56adad108dcb9c676d6613eb289f20988e1ef8211dfab48da94a6782a2a02622
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0-cublas-cuda12-core@sha256:26a399963615f05e98e6a1d03142fb9a34da5899556e424eec3f7d6ca56a74f2
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.24.2-cublas-cuda12-ffmpeg-core@sha256:d5f6b956ccafb6c58eed988c8cd699c5fcbc78af428987d0d59f6acfb27e6090
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0-cublas-cuda11-core@sha256:07c6e5699bd4242d54aaa1d61fd633d63b76780b7b4c266e08ef409da1a896e2
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0-cublas-cuda11-ffmpeg-core@sha256:df04e93b10787c702047792b81920d202d4ca5bf8ad1a5a4bd4e5045899bdc45
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0-aio-gpu-nvidia-cuda-12@sha256:1d517dbfed3445973a90b328659b9c51b066bec42af243eb43376c63c2d93b84
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.25.0-aio-gpu-nvidia-cuda-11@sha256:5ea947a9d79f69db9e7af889bdfbf1e7b2e66de34f65ab0f485e168597651dba
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.24.2-aio-cpu@sha256:d851da8788da823eed61d5300bc68668f145147e2f4a3d5705b4cd4db4a779a6
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
